{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-hawaiian",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "The likelihood function conditional on the data is:\n",
    "\n",
    "$\n",
    "\\mathbb{P}(\\mathbf{X}|p) = p^{\\mathbf{1}^T\\mathbf{X}}(1-p)^{N-\\mathbf{1}^T\\mathbf{X}}\n",
    "$\n",
    "\n",
    "Take the log-likelihood of the function and obtain the MLE of $p^*$:\n",
    "\n",
    "$\n",
    "\\mathcal{l}(p) = \\mathbf{1}^T\\mathbf{X}\\log(p) + (N-\\mathbf{1}^T\\mathbf{X})\\log(1-p)\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial\\mathcal{l}}{\\partial p} = \\frac{\\mathbf{1}^T\\mathbf{X}}{p} - \\frac{N-\\mathbf{1}^T\\mathbf{X}}{1-p} = 0\n",
    "$\n",
    "\n",
    "$\n",
    "p^*N = \\mathbf{1}^T\\mathbf{X}\n",
    "$\n",
    "\n",
    "$\n",
    "p^*_{MLE} = \\frac{\\mathbf{1}^T\\mathbf{X}}{N} = \\frac{1}{N}\\sum_{i=1}^N x_i\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-container",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "The likelihood function of posterior is:\n",
    "\n",
    "$\n",
    "\\mathbb{P}(p|\\mathbf{X}) \\propto \\mathbb{P}(\\mathbf{X}|p) \\mathbb{P}(p) \\propto \n",
    "p^{\\mathbf{1}^T\\mathbf{X}+\\alpha-1}(1-p)^{N-\\mathbf{1}^T\\mathbf{X}+\\beta-1}\n",
    "$\n",
    "\n",
    "We assume that $\\alpha > \\beta > 1$. Therefore, the posterior thus follows Beta distribution: Beta( $\\mathbf{1}^T\\mathbf{X}+\\alpha$, $N-\\mathbf{1}^T\\mathbf{X}+\\beta$ )\n",
    "\n",
    "To maximize the posterior likelihood, we take the mode of the beta distribution:\n",
    "\n",
    "$p^*_{MAP} = \\frac{\\mathbf{1}^T\\mathbf{X}+\\alpha-1}{N+\\alpha+\\beta-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-experience",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "### (a) \n",
    "\n",
    "Notice that, in expectation, $\\mathbf{1}^T\\mathbf{X} = Np^* = 0.99N$. However, each $x \\in \\{0, 1\\}$, so if $N<100$, then $\\hat{p}_{MLE} = 1$. If $N = 100$, in expectation, $\\hat{p}_{MLE} = 0.99$, which is within $0.01$ of $p^*$.\n",
    "\n",
    "### (b)\n",
    "\n",
    "Take $\\alpha=7, \\beta=2$. Let $p^*_{MAP} = \\frac{\\mathbf{1}^T\\mathbf{X}+\\alpha-1}{N+\\alpha+\\beta-2} > 0.98$, we can obtain $N = 44$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-crash",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "### (a)\n",
    "Use the similar arguments as Q3, we can see that $\\hat{p}_{MLE}$ does not depend on the prior belief. Therefore, we obtain the same answer: $N = 100$.\n",
    "\n",
    "### (b)\n",
    "Take $\\alpha=7, \\beta=2$. Let $p^*_{MAP} = \\frac{\\mathbf{1}^T\\mathbf{X}+\\alpha-1}{N+\\alpha+\\beta-2} < 0.02$, we can obtain $N = 294$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-header",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "In conclusion, when we do have the correct prior, $\\hat{p}_{MAP}$ converges much faster to the true $p^*$ than $\\hat{p}_{MLE}$. However, if the prior is incorrect, then it takes extra samples to correct the prior belief. Therefore, $\\hat{p}_{MLE}$ performs better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
