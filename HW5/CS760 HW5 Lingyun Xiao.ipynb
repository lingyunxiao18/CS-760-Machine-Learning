{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "\n",
    "data = pd.read_csv(\"titanic_data.csv\")\n",
    "data = np.array(data)\n",
    "\n",
    "X_Bernoulli = data[:,[0, 2]]\n",
    "X_Multinomial = data[:, [0, 1, 4, 5]]\n",
    "X_Gaussian = data[:, [0, 3, 6]]\n",
    "X = data[:,1:]\n",
    "Y = data[:,0].astype(int)\n",
    "\n",
    "y_counter = collections.Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data set\n",
    "from sklearn import preprocessing\n",
    "normalized_data = preprocessing.normalize(X, norm='l2')\n",
    "normalized_data = np.concatenate(Y, normalized_data)\n",
    "\n",
    "# Calculate the distance between two points\n",
    "def distance(x1, x2):\n",
    "    distance = 0\n",
    "    for i in range(len(x1)-1):\n",
    "        distance += (x1[i] - x2[i])**2\n",
    "    return math.sqrt(distance)\n",
    " \n",
    "# Select the k nearest neighbors\n",
    "def neighbors(data, vector, k):\n",
    "    distances = list()\n",
    "    for row in data:\n",
    "        d = distance(vector, row)\n",
    "        distances.append((row, d))\n",
    "    distances.sort(key = lambda x: x[1]) # Sort by the distance\n",
    "    neighbors = list()\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    " \n",
    "# Make a classification prediction with neighbors\n",
    "def predict(data, vector, k):\n",
    "    n = neighbors(data, vector, k)\n",
    "    output_values = [row[0] for row in n]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "vector = [3, 1, 30, 0, 0, 100]\n",
    "\n",
    "for k in range(800):\n",
    "    predict(normalized_data, vector, k+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-backing",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "(b) I used Euclidean distance, but on $\\textit{normalized}$ data. The Euclidean distance between two points is the most straightforward way to see if two points are close to or far from each other. However, in order to avoid the bias from weights placed on different features with vastly discrepant mean and variance, using normalized data is essential.\n",
    "\n",
    "(c) The result shows that I would have survived the Titanic sinking given the vector $[3, 1, 30, 0, 0, 100]$.\n",
    "\n",
    "(d) I would choose $k = 8$ because it is not too small to be unstable, and it gives the highest success rate of prediction.\n",
    "\n",
    "(e) I would use cross validation to assess the confidence level. I can compare different KNNs for various K and find the one with the highest accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prior\n",
    "def prior(X, Y):\n",
    "    m, n = X.shape\n",
    "    label = np.unique(Y)\n",
    "    prior_list = np.zeros(2)\n",
    "    for i in range(m):\n",
    "        label = Y[i]\n",
    "        prior_list[int(label)] += 1\n",
    "    prior_list = prior_list/m\n",
    "    return prior_list\n",
    "\n",
    "def Bernoulli_likelihood(vector, y):\n",
    "    # Calculate the Bernoulli conditional probability\n",
    "    nom = 0\n",
    "    for i in range(len(data)):\n",
    "        x = vector[1]\n",
    "        if np.all(X_Bernoulli[i] == [y, x]):\n",
    "            nom += 1\n",
    "            \n",
    "    denom = y_counter[y]\n",
    "    return nom/denom\n",
    "\n",
    "def Multinomial_likelihood(vector, y):\n",
    "    \n",
    "    # Select the columns with multinomial distribution\n",
    "    X = list(i for i in vector if i in [0,3,4])\n",
    "    \n",
    "    # Calculate the Multinomial conditional probability\n",
    "    nom = [0, 0, 0]\n",
    "    for idx in range(len(X)):\n",
    "        x = X[idx]\n",
    "        for j in range(len(data)):\n",
    "            if np.all(X_Multinomial[j, [0, idx+1]] == [y, x]):\n",
    "                nom[idx] += 1\n",
    "                \n",
    "    denom = y_counter[y]\n",
    "    likelihoods = [nom[idx]/denom for idx in range(len(X))]\n",
    "    return likelihoods # Return a list of likelihoods\n",
    "    \n",
    "def normal_pdf(x, mean, var):\n",
    "    denom = (2*math.pi*var)**.5\n",
    "    num = math.exp(-(float(x)-float(mean))**2/(2*var))\n",
    "    return num/denom\n",
    "\n",
    "def Gaussian_likelihood(vector, y):\n",
    "    \n",
    "    # Select the columns with Gaussian distribution\n",
    "    X = list(i for i in vector if i in [2, 5])\n",
    "    \n",
    "    # Calculate the Gaussian conditional probability\n",
    "    likelihoods = []\n",
    "    for idx in range(len(X)):\n",
    "        x = X[idx]\n",
    "        for j in range(len(y_counter)):\n",
    "            mean = np.mean(X_Gaussian[X_Gaussian[:, 0] == j, :], axis = 0)[idx+1]\n",
    "            var = np.var(X_Gaussian[X_Gaussian[:, 0] == j, :], axis = 0)[idx+1]\n",
    "            likelihoods.append(normal_pdf(x, mean, var))\n",
    "    return likelihoods # Return a list of likelihoods\n",
    "\n",
    "def posterior(y):\n",
    "    post = prior(X,Y)[y]*Bernoulli_likelihood(vector, 0)*np.prod(Multinomial_likelihood(vector, y))*np.prod(Gaussian_likelihood(vector, y))\n",
    "    return post\n",
    "\n",
    "def predict(vector):\n",
    "    post = []\n",
    "    for i in range(len(y.counter)):\n",
    "        y = y.counter[i]\n",
    "        post[i] = posterior(y)\n",
    "    result = np.argmax(post)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [3, 1, 30, 0, 0, 100]\n",
    "predict(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-amplifier",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "(b) \n",
    "\n",
    "For Bernoulli and Multinomial, calculation of likelihoods is relatively simple. I counted the number of instances (e.g. $x_1 = 0, y = 0$) occured, and divide it by the total number of $y=0$ occured. \n",
    "\n",
    "For Gaussian, I first obtained the mean and variance of each instance, and then calculate the Gaussian density.\n",
    "\n",
    "The first two are discrete distributions, and the last is continuous. \n",
    "\n",
    "(c)\n",
    "\n",
    "The result shows that I would have survived the Titanic sinking given the vector $[3, 1, 30, 0, 0, 100]$. \n",
    "\n",
    "(d)\n",
    "\n",
    "I could use k-fold cross validation to test the accuracy of my Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-serial",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "I would prefer Random Forests because it could handle large scale data set, and also categorical data really well, which is the case we have. Also, the features in Titanic data set is highly likely to be correlated, so other methods would suffer much more from the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253aefa",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "$\\hat{P}(y=ham|x) \\propto \\hat{P}(y=ham) \\times \\Pi_{j=1}^D \\hat{P}(x_j|y=ham)\n",
    "= \\frac{2}{5}\\times \\frac{1}{4} \\times \\frac{1}{2} \\times \\frac{1}{4} \\times \\frac{1}{4} \\times \\frac{1}{4} \n",
    "\\times \\frac{1}{4} \\times \\frac{1}{2} \\times \\frac{1}{2} \\approx 4.9\\times 10^{-5} << 0.0289\n",
    "$\n",
    "\n",
    "Therefore, the email is classified as Spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-outline",
   "metadata": {},
   "source": [
    "## Q5\n",
    "$\n",
    "\\hat{P}(y=female|x) \\propto \\hat{P}(y=female) \\times \\Pi_{j=1}^D \\hat{P}(x_j|y=female)\n",
    "= \\frac{1}{3} \\frac{1}{\\sqrt{2\\pi(2)}}e^{-\\frac{(42-38)^2}{2(2)}} \n",
    " \\frac{1}{\\sqrt{2\\pi(50)}}e^{-\\frac{(180-165)^2}{2(50)}} \n",
    " \\frac{1}{\\sqrt{2\\pi(0.125)}}e^{-\\frac{(5.5-6.75)^2}{2(0.125)}} \n",
    "$\n",
    "\n",
    "$\n",
    "\\approx 1.35 \\times 10^{-8} < 6.4745 \\times 10^{âˆ’4}\n",
    "$\n",
    "\n",
    "Therefore, the suspect is classified as Male."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
